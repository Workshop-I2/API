{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c92f0a96-694c-4656-b1e2-89454a14b0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\epsi\\i2\\workshop\\api\\env\\lib\\site-packages (4.45.2)\n",
      "Requirement already satisfied: torch in d:\\epsi\\i2\\workshop\\api\\env\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: filelock in d:\\epsi\\i2\\workshop\\api\\env\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in d:\\epsi\\i2\\workshop\\api\\env\\lib\\site-packages (from transformers) (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\epsi\\i2\\workshop\\api\\env\\lib\\site-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\epsi\\i2\\workshop\\api\\env\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\epsi\\i2\\workshop\\api\\env\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\epsi\\i2\\workshop\\api\\env\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in d:\\epsi\\i2\\workshop\\api\\env\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in d:\\epsi\\i2\\workshop\\api\\env\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in d:\\epsi\\i2\\workshop\\api\\env\\lib\\site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\epsi\\i2\\workshop\\api\\env\\lib\\site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\epsi\\i2\\workshop\\api\\env\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in d:\\epsi\\i2\\workshop\\api\\env\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in d:\\epsi\\i2\\workshop\\api\\env\\lib\\site-packages (from torch) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in d:\\epsi\\i2\\workshop\\api\\env\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in d:\\epsi\\i2\\workshop\\api\\env\\lib\\site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: colorama in d:\\epsi\\i2\\workshop\\api\\env\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\epsi\\i2\\workshop\\api\\env\\lib\\site-packages (from jinja2->torch) (3.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\epsi\\i2\\workshop\\api\\env\\lib\\site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\epsi\\i2\\workshop\\api\\env\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\epsi\\i2\\workshop\\api\\env\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\epsi\\i2\\workshop\\api\\env\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\epsi\\i2\\workshop\\api\\env\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "488ab5be-64cd-4746-98e0-1ce51f193ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e8e1d85-b70f-4ade-84c9-88e0ef113d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at GroNLP/hateBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Charger le modèle HateBERT et son tokenizer depuis Hugging Face\n",
    "model_name = \"GroNLP/hateBERT\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5704add9-4b16-4ecf-ad48-68e2b0703d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir le texte à analyser (exemple de post de réseau social)\n",
    "texts = [\n",
    "    \"I hate everyone in this place!\",\n",
    "    \"You are such a nice person!\",\n",
    "    \"All these people are disgusting and should leave.\",\n",
    "    \"Gays should not be allowed\",\n",
    "    \"Gays are nice people\",\n",
    "    \"All black people must die\",\n",
    "    \"All white people must die\",\n",
    "    \"I love mexican food\",\n",
    "    \"I love mexicans\",\n",
    "    \"Fuck you bitch\",\n",
    "    \"You talking to me mother fucker\",\n",
    "    \"Listen to me you litle monkey\",\n",
    "    \"I want to lick your pussy and then fuck you\",\n",
    "    \"you blakc bastard\",\n",
    "    \"Burn the jewish\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d37940f8-58ad-4dbd-bb49-e577d51cdee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2c8a66e3-e281-4c54-8bc1-ff3c7bfb6fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6a46d46b-2ccb-4fdd-b4bc-e5b87ba5b5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = torch.softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ae95f485-02d7-4c0c-b75f-93824b2f86d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte: I hate everyone in this place!\n",
      "Probabilité (non toxique, toxique) : [0.52955616 0.47044384]\n",
      "Classifié comme non toxique\n",
      "--------------------------------------------------\n",
      "Texte: You are such a nice person!\n",
      "Probabilité (non toxique, toxique) : [0.49630985 0.5036902 ]\n",
      "Classifié comme toxique\n",
      "--------------------------------------------------\n",
      "Texte: All these people are disgusting and should leave.\n",
      "Probabilité (non toxique, toxique) : [0.53720784 0.4627921 ]\n",
      "Classifié comme non toxique\n",
      "--------------------------------------------------\n",
      "Texte: Gays should not be allowed\n",
      "Probabilité (non toxique, toxique) : [0.6060882 0.3939118]\n",
      "Classifié comme non toxique\n",
      "--------------------------------------------------\n",
      "Texte: Gays are nice people\n",
      "Probabilité (non toxique, toxique) : [0.57921237 0.42078754]\n",
      "Classifié comme non toxique\n",
      "--------------------------------------------------\n",
      "Texte: All black people must die\n",
      "Probabilité (non toxique, toxique) : [0.48308858 0.5169115 ]\n",
      "Classifié comme toxique\n",
      "--------------------------------------------------\n",
      "Texte: All white people must die\n",
      "Probabilité (non toxique, toxique) : [0.48320132 0.5167987 ]\n",
      "Classifié comme toxique\n",
      "--------------------------------------------------\n",
      "Texte: I love mexican food\n",
      "Probabilité (non toxique, toxique) : [0.51687455 0.48312542]\n",
      "Classifié comme non toxique\n",
      "--------------------------------------------------\n",
      "Texte: I love mexicans\n",
      "Probabilité (non toxique, toxique) : [0.54884416 0.4511558 ]\n",
      "Classifié comme non toxique\n",
      "--------------------------------------------------\n",
      "Texte: Fuck you bitch\n",
      "Probabilité (non toxique, toxique) : [0.50688195 0.49311808]\n",
      "Classifié comme non toxique\n",
      "--------------------------------------------------\n",
      "Texte: You talking to me mother fucker\n",
      "Probabilité (non toxique, toxique) : [0.54828084 0.45171914]\n",
      "Classifié comme non toxique\n",
      "--------------------------------------------------\n",
      "Texte: Listen to me you litle monkey\n",
      "Probabilité (non toxique, toxique) : [0.49591017 0.50408983]\n",
      "Classifié comme toxique\n",
      "--------------------------------------------------\n",
      "Texte: I want to lick your pussy and then fuck you\n",
      "Probabilité (non toxique, toxique) : [0.56247497 0.43752503]\n",
      "Classifié comme non toxique\n",
      "--------------------------------------------------\n",
      "Texte: you blakc bastard\n",
      "Probabilité (non toxique, toxique) : [0.48084202 0.51915795]\n",
      "Classifié comme toxique\n",
      "--------------------------------------------------\n",
      "Texte: Burn the jewish\n",
      "Probabilité (non toxique, toxique) : [0.47698706 0.52301294]\n",
      "Classifié comme toxique\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Afficher les résultats\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"Texte: {text}\")\n",
    "    print(f\"Probabilité (non toxique, toxique) : {probabilities[i].numpy()}\")\n",
    "    predicted_class = torch.argmax(probabilities[i]).item()\n",
    "    if predicted_class == 1:\n",
    "        print(\"Classifié comme toxique\")\n",
    "    else:\n",
    "        print(\"Classifié comme non toxique\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6e7ecb32-b05a-4697-905d-d97f3a1dbd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test avec le model roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e8df5ebb-9d58-4874-89e8-72902998f550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "949e1799eb334cec99b6ef1385a5f38c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/652 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Epsi\\I2\\workshop\\API\\env\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\flori\\.cache\\huggingface\\hub\\models--cardiffnlp--twitter-xlm-roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eae017bf9cd44cdea83456fb26b03fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dba5b96489b14eccaa812200f0bed312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d71740d0d454d94a7be2f101603ca3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"cardiffnlp/twitter-xlm-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d3aeedb4-63d4-4500-9d60-58be5d0eeefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"I hate everyone in this place!\",\n",
    "    \"You are such a nice person!\",\n",
    "    \"All these people are disgusting and should leave.\",\n",
    "    \"Gays should not be allowed\",\n",
    "    \"Gays are nice people\",\n",
    "    \"All black people must die\",\n",
    "    \"All white people must die\",\n",
    "    \"I love mexican food\",\n",
    "    \"I love mexicans\",\n",
    "    \"Fuck you bitch\",\n",
    "    \"You talking to me mother fucker\",\n",
    "    \"Listen to me you litle monkey\",\n",
    "    \"I want to lick your pussy and then fuck you\",\n",
    "    \"you blakc bastard\",\n",
    "    \"Burn the jewish\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b1f6d343-342e-474a-9696-8dfe87eedc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "563593eb-470c-489f-b7c6-4a1a5605c2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faire des prédictions avec le modèle\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# Convertir les logits en probabilités\n",
    "probabilities = torch.softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9a9fa1a1-f9af-4be8-b665-3ffd2c2ff49d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte: I hate everyone in this place!\n",
      "Probabilité (non toxique, toxique) : [0.4896219 0.510378 ]\n",
      "Classifié comme toxique\n",
      "--------------------------------------------------\n",
      "Texte: You are such a nice person!\n",
      "Probabilité (non toxique, toxique) : [0.4976875  0.50231254]\n",
      "Classifié comme toxique\n",
      "--------------------------------------------------\n",
      "Texte: All these people are disgusting and should leave.\n",
      "Probabilité (non toxique, toxique) : [0.49864268 0.5013573 ]\n",
      "Classifié comme toxique\n",
      "--------------------------------------------------\n",
      "Texte: Gays should not be allowed\n",
      "Probabilité (non toxique, toxique) : [0.49135995 0.50864   ]\n",
      "Classifié comme toxique\n",
      "--------------------------------------------------\n",
      "Texte: Gays are nice people\n",
      "Probabilité (non toxique, toxique) : [0.49867344 0.5013265 ]\n",
      "Classifié comme toxique\n",
      "--------------------------------------------------\n",
      "Texte: All black people must die\n",
      "Probabilité (non toxique, toxique) : [0.4965412 0.5034588]\n",
      "Classifié comme toxique\n",
      "--------------------------------------------------\n",
      "Texte: All white people must die\n",
      "Probabilité (non toxique, toxique) : [0.49740145 0.50259846]\n",
      "Classifié comme toxique\n",
      "--------------------------------------------------\n",
      "Texte: I love mexican food\n",
      "Probabilité (non toxique, toxique) : [0.5020039 0.4979961]\n",
      "Classifié comme non toxique\n",
      "--------------------------------------------------\n",
      "Texte: I love mexicans\n",
      "Probabilité (non toxique, toxique) : [0.49825788 0.5017421 ]\n",
      "Classifié comme toxique\n",
      "--------------------------------------------------\n",
      "Texte: Fuck you bitch\n",
      "Probabilité (non toxique, toxique) : [0.5036025  0.49639747]\n",
      "Classifié comme non toxique\n",
      "--------------------------------------------------\n",
      "Texte: You talking to me mother fucker\n",
      "Probabilité (non toxique, toxique) : [0.5043361  0.49566382]\n",
      "Classifié comme non toxique\n",
      "--------------------------------------------------\n",
      "Texte: Listen to me you litle monkey\n",
      "Probabilité (non toxique, toxique) : [0.49775568 0.50224435]\n",
      "Classifié comme toxique\n",
      "--------------------------------------------------\n",
      "Texte: I want to lick your pussy and then fuck you\n",
      "Probabilité (non toxique, toxique) : [0.50126874 0.49873123]\n",
      "Classifié comme non toxique\n",
      "--------------------------------------------------\n",
      "Texte: you blakc bastard\n",
      "Probabilité (non toxique, toxique) : [0.51892865 0.4810714 ]\n",
      "Classifié comme non toxique\n",
      "--------------------------------------------------\n",
      "Texte: Burn the jewish\n",
      "Probabilité (non toxique, toxique) : [0.49314025 0.5068598 ]\n",
      "Classifié comme toxique\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Afficher les résultats\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"Texte: {text}\")\n",
    "    print(f\"Probabilité (non toxique, toxique) : {probabilities[i].numpy()}\")\n",
    "    predicted_class = torch.argmax(probabilities[i]).item()\n",
    "    if predicted_class == 1:\n",
    "        print(\"Classifié comme toxique\")\n",
    "    else:\n",
    "        print(\"Classifié comme non toxique\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6c76d852-4ad1-45e2-a349-ce68414f096a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "301f2ba5cfa84355acd98ec7c91a72a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/174 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Epsi\\I2\\workshop\\API\\env\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\flori\\.cache\\huggingface\\hub\\models--unitary--toxic-bert. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "047e663cfacb4810a93f3beab15cdd3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/811 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e2b5ddc951b43c5b84dfe94c3132e84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f55eab985e146118a4053ab2229cfd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aa49f78b1444eb6904b936fe4cce404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"unitary/toxic-bert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b77f9ea6-fe09-4567-97ff-f30d9c2c4b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"I hate everyone in this place!\",\n",
    "    \"You are such a nice person!\",\n",
    "    \"All these people are disgusting and should leave.\",\n",
    "    \"Gays should not be allowed\",\n",
    "    \"Gays are nice people\",\n",
    "    \"All black people must die\",\n",
    "    \"All white people must die\",\n",
    "    \"I love mexican food\",\n",
    "    \"I love mexicans\",\n",
    "    \"Fuck you bitch\",\n",
    "    \"You talking to me mother fucker\",\n",
    "    \"Listen to me you litle monkey\",\n",
    "    \"I want to lick your pussy and then fuck you\",\n",
    "    \"you black bastard\",\n",
    "    \"Burn the jewish\",\n",
    "    \"Brulez les noirs\",\n",
    "    \"Les gays sont des personnes comme les autres\",\n",
    "    \"J'ai acheté un pantalon noir qui est magnifique\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "57ab9864-fd9d-48ac-85b1-e62d8b69c63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3a329c0f-1fee-4e26-ab76-73f9364f8f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "# Convertir les logits en probabilités\n",
    "probabilities = torch.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "022a29c8-ddfa-40e5-b88c-1500ac5d6eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte: I hate everyone in this place!\n",
      "Probabilités (toxicité, insultes, langage haineux, etc.) : [0.8219915  0.00233375 0.00933863 0.00986862 0.01807629 0.01598729]\n",
      "toxique: 0.82\n",
      "insultes: 0.00\n",
      "menaces: 0.01\n",
      "langage haineux: 0.01\n",
      "sexisme: 0.02\n",
      "racisme: 0.02\n",
      "--------------------------------------------------\n",
      "Texte: You are such a nice person!\n",
      "Probabilités (toxicité, insultes, langage haineux, etc.) : [0.00100335 0.00011819 0.00021778 0.00012186 0.00019738 0.00015175]\n",
      "toxique: 0.00\n",
      "insultes: 0.00\n",
      "menaces: 0.00\n",
      "langage haineux: 0.00\n",
      "sexisme: 0.00\n",
      "racisme: 0.00\n",
      "--------------------------------------------------\n",
      "Texte: All these people are disgusting and should leave.\n",
      "Probabilités (toxicité, insultes, langage haineux, etc.) : [0.9166148  0.004037   0.02027821 0.00447838 0.21530707 0.09899177]\n",
      "toxique: 0.92\n",
      "insultes: 0.00\n",
      "menaces: 0.02\n",
      "langage haineux: 0.00\n",
      "sexisme: 0.22\n",
      "racisme: 0.10\n",
      "--------------------------------------------------\n",
      "Texte: Gays should not be allowed\n",
      "Probabilités (toxicité, insultes, langage haineux, etc.) : [0.88544726 0.02306193 0.07402731 0.01262651 0.1313098  0.6636025 ]\n",
      "toxique: 0.89\n",
      "insultes: 0.02\n",
      "menaces: 0.07\n",
      "langage haineux: 0.01\n",
      "sexisme: 0.13\n",
      "racisme: 0.66\n",
      "--------------------------------------------------\n",
      "Texte: Gays are nice people\n",
      "Probabilités (toxicité, insultes, langage haineux, etc.) : [0.77945787 0.01501863 0.05576162 0.00568565 0.11989031 0.5914633 ]\n",
      "toxique: 0.78\n",
      "insultes: 0.02\n",
      "menaces: 0.06\n",
      "langage haineux: 0.01\n",
      "sexisme: 0.12\n",
      "racisme: 0.59\n",
      "--------------------------------------------------\n",
      "Texte: All black people must die\n",
      "Probabilités (toxicité, insultes, langage haineux, etc.) : [0.986878   0.45645836 0.46171707 0.85102236 0.6158269  0.91080135]\n",
      "toxique: 0.99\n",
      "insultes: 0.46\n",
      "menaces: 0.46\n",
      "langage haineux: 0.85\n",
      "sexisme: 0.62\n",
      "racisme: 0.91\n",
      "--------------------------------------------------\n",
      "Texte: All white people must die\n",
      "Probabilités (toxicité, insultes, langage haineux, etc.) : [0.9827328  0.39451045 0.34695923 0.85396636 0.525601   0.88255095]\n",
      "toxique: 0.98\n",
      "insultes: 0.39\n",
      "menaces: 0.35\n",
      "langage haineux: 0.85\n",
      "sexisme: 0.53\n",
      "racisme: 0.88\n",
      "--------------------------------------------------\n",
      "Texte: I love mexican food\n",
      "Probabilités (toxicité, insultes, langage haineux, etc.) : [0.00201341 0.00011683 0.0001968  0.00013954 0.00019345 0.00028613]\n",
      "toxique: 0.00\n",
      "insultes: 0.00\n",
      "menaces: 0.00\n",
      "langage haineux: 0.00\n",
      "sexisme: 0.00\n",
      "racisme: 0.00\n",
      "--------------------------------------------------\n",
      "Texte: I love mexicans\n",
      "Probabilités (toxicité, insultes, langage haineux, etc.) : [0.01558757 0.0002295  0.00052052 0.00049578 0.00059579 0.00189266]\n",
      "toxique: 0.02\n",
      "insultes: 0.00\n",
      "menaces: 0.00\n",
      "langage haineux: 0.00\n",
      "sexisme: 0.00\n",
      "racisme: 0.00\n",
      "--------------------------------------------------\n",
      "Texte: Fuck you bitch\n",
      "Probabilités (toxicité, insultes, langage haineux, etc.) : [0.9977986  0.4751979  0.9930226  0.00373083 0.9707394  0.02876976]\n",
      "toxique: 1.00\n",
      "insultes: 0.48\n",
      "menaces: 0.99\n",
      "langage haineux: 0.00\n",
      "sexisme: 0.97\n",
      "racisme: 0.03\n",
      "--------------------------------------------------\n",
      "Texte: You talking to me mother fucker\n",
      "Probabilités (toxicité, insultes, langage haineux, etc.) : [0.9973247  0.33585846 0.989584   0.0033704  0.8894593  0.00745454]\n",
      "toxique: 1.00\n",
      "insultes: 0.34\n",
      "menaces: 0.99\n",
      "langage haineux: 0.00\n",
      "sexisme: 0.89\n",
      "racisme: 0.01\n",
      "--------------------------------------------------\n",
      "Texte: Listen to me you litle monkey\n",
      "Probabilités (toxicité, insultes, langage haineux, etc.) : [0.88946843 0.00400931 0.13106534 0.00128085 0.64338106 0.01388653]\n",
      "toxique: 0.89\n",
      "insultes: 0.00\n",
      "menaces: 0.13\n",
      "langage haineux: 0.00\n",
      "sexisme: 0.64\n",
      "racisme: 0.01\n",
      "--------------------------------------------------\n",
      "Texte: I want to lick your pussy and then fuck you\n",
      "Probabilités (toxicité, insultes, langage haineux, etc.) : [0.9975108  0.46591628 0.9926529  0.01476637 0.761437   0.00878401]\n",
      "toxique: 1.00\n",
      "insultes: 0.47\n",
      "menaces: 0.99\n",
      "langage haineux: 0.01\n",
      "sexisme: 0.76\n",
      "racisme: 0.01\n",
      "--------------------------------------------------\n",
      "Texte: you black bastard\n",
      "Probabilités (toxicité, insultes, langage haineux, etc.) : [0.99763775 0.5735157  0.98395336 0.03290372 0.97921556 0.81333846]\n",
      "toxique: 1.00\n",
      "insultes: 0.57\n",
      "menaces: 0.98\n",
      "langage haineux: 0.03\n",
      "sexisme: 0.98\n",
      "racisme: 0.81\n",
      "--------------------------------------------------\n",
      "Texte: Burn the jewish\n",
      "Probabilités (toxicité, insultes, langage haineux, etc.) : [0.9646783  0.15803376 0.17358293 0.39829075 0.27717155 0.86366934]\n",
      "toxique: 0.96\n",
      "insultes: 0.16\n",
      "menaces: 0.17\n",
      "langage haineux: 0.40\n",
      "sexisme: 0.28\n",
      "racisme: 0.86\n",
      "--------------------------------------------------\n",
      "Texte: Brulez les noirs\n",
      "Probabilités (toxicité, insultes, langage haineux, etc.) : [0.00961684 0.00011335 0.00048846 0.0001311  0.00049294 0.00025646]\n",
      "toxique: 0.01\n",
      "insultes: 0.00\n",
      "menaces: 0.00\n",
      "langage haineux: 0.00\n",
      "sexisme: 0.00\n",
      "racisme: 0.00\n",
      "--------------------------------------------------\n",
      "Texte: Les gays sont des personnes comme les autres\n",
      "Probabilités (toxicité, insultes, langage haineux, etc.) : [0.8336268  0.0140946  0.05951131 0.00679824 0.11213716 0.51675236]\n",
      "toxique: 0.83\n",
      "insultes: 0.01\n",
      "menaces: 0.06\n",
      "langage haineux: 0.01\n",
      "sexisme: 0.11\n",
      "racisme: 0.52\n",
      "--------------------------------------------------\n",
      "Texte: J'ai acheté un pantalon noir qui est magnifique\n",
      "Probabilités (toxicité, insultes, langage haineux, etc.) : [0.0278594  0.00013522 0.00082365 0.00023882 0.00080082 0.00048157]\n",
      "toxique: 0.03\n",
      "insultes: 0.00\n",
      "menaces: 0.00\n",
      "langage haineux: 0.00\n",
      "sexisme: 0.00\n",
      "racisme: 0.00\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Afficher les résultats\n",
    "for i, text in enumerate(texts):\n",
    "    print(f\"Texte: {text}\")\n",
    "    print(f\"Probabilités (toxicité, insultes, langage haineux, etc.) : {probabilities[i].numpy()}\")\n",
    "    toxic_categories = ['toxique', 'insultes', 'menaces', 'langage haineux', 'sexisme', 'racisme']\n",
    "    for j, category in enumerate(toxic_categories):\n",
    "        print(f\"{category}: {probabilities[i][j].item():.2f}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0e38cc-73c3-4914-832b-1e649205d4e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572cee69-202a-4871-8618-566953f5764b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d6933a-c7ae-4b84-a4f8-c05d23e7928f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
